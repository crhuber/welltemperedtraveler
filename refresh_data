#!/usr/bin/python3

import json
import os
import subprocess
import sys

from optparse import OptionParser

USE_NEW_DATA_SOURCE = False
DEBUG = False
# https://en.wikipedia.org/wiki/ISO_3166-1
LOCATIONS = [
  # https://en.wikipedia.org/wiki/List_of_national_capitals_in_alphabetical_order
  #
  # https://en.wikipedia.org/wiki/List_of_cities_proper_by_population
  # Next: 41
  "Abu Dhabi|AE",
  "Abuja|NG",
  "Accra|GH",
  "Adamstown|PN",
  "Addis Ababa|ET",
  "Algiers|DZ",
  #"Alofi|NU",
  "Amman|JO",
  #"Amsterdam|NL",
  "Anchorage|US",
  #"Andorra la Vella|AD",
  "Ankara|TR",
  "Antananarivo|MG",
  #"Apia|WS",
  "Ashgabat|TM",
  "Asmara|ER",
  "Astana|KZ",
  #"Asunción|PY",
  #"Athens|GR",
  "Atlanta|US",
  "Auckland|NZ",
  "Austin|US",
  #"Avarua|CK",
  "Baghdad|IQ",
  "Baku|AZ",
  "Bamako|ML",
  "Bandar Seri Begawan|BN",
  "Bangkok|TH",
  "Bangui|CF",
  #"Banjul|GM",
  "Barcelona|ES",
  #"Basse-Terre|GP",
  "Basseterre|KN",
  "Beijing|CN",
  #"Beirut|LB",
  "Belgrade|RS",
  #"Belmopan|BZ",
  "Berlin|DE",
  "Bengaluru|IN",
  #"Berne|CH",
  "Bishkek|KG",
  "Bissau|GW",
  "Bogotá|CO",
  "Boston|US",
  "Boulder|US",
  "Brasília|BR",
  "Bratislava|SK",
  "Brazzaville|CG",
  #"Brussels|BE",
  #"Bridgetown|BB",
  "Bucharest|RO",
  "Budapest|HU",
  "Buenos Aires|AR",
  #"Cairo|EG",
  "Canberra|AU",
  "Caracas|VE",
  "Changsha|CN",
  "Chengdu|CN",
  "Chennai|IN",
  "Chicago|US",
  "Chongqing|CN",
  #"Copenhagen|DK",
  "Dakar|SN",
  "Dallas|US",
  #"Damascus|SY",
  "Denver|US",
  "Dhaka|BD",
  #"Dongguan|CN",
  "Dublin|IE",
  "New Delhi|IN",
  "Dhaka|BD",
  "Florence|IT",
  "Frankfurt|DE",
  "Guangzhou|CN",
  #"Hà Nội|VN",
  #"Hải Phòng|VN",
  "Havana|CU",
  "Helsinki|FI",
  #"Hồ Chí Minh City|VN",
  "Hong Kong|HK",
  "Honolulu|US",
  "Houston|US",
  "Hyderabad|IN",
  "Islamabad|PK",
  "Istanbul|TR",
  "Jakarta|ID",
  #"Johannesburg|ZA",
  "Kabul|AF",
  "Kansas City|US",
  "Karachi|PK",
  "Kathmandu|NP",
  #"Kiev|UA",
  "Kinshasa|CD",
  "Kobe|JP",
  #"Krakow|PL",
  "Kyoto|JP",
  #"La Paz|BO",
  "Lahore|PK",
  "Lagos|NG",
  "Las Vegas|US",
  "Libreville|GA",
  "Lima|PE",
  "Lisbon|PT",
  "Loja|EC",
  "London|GB",
  "Los Angeles|US",
  "Madrid|ES",
  "Manila|PH",
  "Marrakesh|MA",
  "Mexico City|MX",
  "Miami|US",
  "Milan|IT",
  "Minneapolis|US",
  "Minsk|BY",
  "Montevideo|UY",
  "Moscow|RU",
  "Mumbai|IN",
  "Munich|DE",
  "Nagoya|JP",
  "Nanjing|CN",
  #"Naples|IT",
  "New York|US",
  "Nice|FR",
  "Ningbo|CN",
  "Nizhny Novgorod|RU",
  "Novosibirsk|RU",
  #"Osaka|JP",
  "Oslo|NO",
  "Paris|FR",
  "Pittsburgh|US",
  "Portland|US",
  "Prague|CZ",
  "Pune|IN",
  #"Rabat|MA",
  "Reykjavík|IS",
  "Riga|LV",
  "Rio de Janeiro|BR",
  "Rome|IT",
  "Saint Petersburg|RU",
  "San Diego|US",
  "San Francisco|US",
  "San José|CR",
  "Santa Barbara|US",
  "Santiago|CL",
  "Sanya|CN",
  "São Paulo|BR",
  "Sapporo|JP",
  "Sarajevo|BA",
  "Seattle|US",
  "Seoul|KR",
  "Shanghai|CN",
  "Shenyang|CN",
  "Shenzhen|CN",
  "Singapore|SG",
  "Sofia|BG",
  "Stockholm|SE",
  "Sydney|AU",
  "Taipei|TW",
  "Tallinn|EE",
  "Tbilisi|GE",
  "Tehran|IR",
  #"Thimphu|BT",
  "Tianjin|CN",
  "Tokyo|JP",
  "Toronto|CA",
  "Ulaanbaatar|MN",
  "Vancouver|CA",
  "Venice|IT",
  "Vienna|AT",
  "Vientiane|LA",
  "Vilnius|LT",
  "Warsaw|PL",
  #"Washington DC|US",
  "Winnipeg|CA",
  "Wuhan|CN",
  "Yekaterinburg|RU",
  "Yerevan|AM",
  "Zagreb|HR",
  #"Zurich|CH",
]

MIN_COMPLETION_RATIO = "1.0"
DATA_FILE = "data.txt"
DREMEL_TEMPLATE = (
'SELECT Date, Location, Min_C, Mean_C, Max_C, Rain, Snow '
#'SELECT Date, Location, Min_C, Mean_C, Max_C, Min_H, Mean_H, Max_H, Rain, Snow '
'FROM '
'  (SELECT date as Date, '
'    station_id as StationId, '
'    temp_c_min as Min_C, '
'    temp_c_mean as Mean_C, '
'    temp_c_max as Max_C, '
#'    humidity_pct_min as Min_H, '
#'    humidity_pct_mean as Mean_H, '
#'    humidity_pct_max as Max_H, '
'    rainfall_mm as Rain, '
'    snowfall_mm as Snow '
'  FROM '
'    weather.historical.daily' + ('.new.capacitor' if USE_NEW_DATA_SOURCE else '') + ' '
'  WHERE '
'    (date >= "%s") AND (date <= "%s") AND '
'    temp_c_max != -9999) AS W '
'JOIN '
'  (SELECT '
'    id as Id, '
'    CONCAT(geo.locality_name, ", ", geo.subdivision_1_name, ", ", geo.region_code) as Location '
'  FROM '
'    weather.stations' + ('.new.capacitor' if USE_NEW_DATA_SOURCE else '') + ' '
'  WHERE '
'    geo.locality_name = "%s" AND '
'    geo.region_code = "%s") AS S '
'ON W.StationId = S.Id '
'ORDER BY Date;')

def has_data(location):
  for half_month in master_data[location]:
    for data_point in master_data[location][half_month]:
      if data_point != -9999:
        return True
  return False

def array_has_data(the_array):
  for a in the_array:
    if a != -9999:
      return True
  return False

parser = OptionParser()
parser.add_option("-f", "--force", dest="force",
                  action="store_true",
                  help="Force refresh existing data")
(options, args) = parser.parse_args()

master_data = {}
if os.path.exists(DATA_FILE):
  lines = open(DATA_FILE).readlines()
  for l in lines:
    parts = l.split(":", 1)
    master_data[parts[0]] = json.loads(parts[1])

start_year = 2009
end_year = 2018
all_years_str = [str(y) for y in range(start_year, end_year + 1)]
start_date = str(start_year) + "0101"
end_date = str(end_year) + "1231"
for location in LOCATIONS:
  (place, region_code) = location.split("|")
  buffer = len(place + ", " + region_code)
  sys.stdout.write(place + ", " + region_code)
  for i in range(0, 40 - buffer):
    sys.stdout.write(".")
  sys.stdout.write(" ")
  if location in master_data:
    if has_data(location) and not options.force:
        sys.stdout.write("✓\n")
        continue
  sys.stdout.flush()
  if place not in master_data:
    master_data[location] = {}
  command = ["echo", "'" + (DREMEL_TEMPLATE % (start_date, end_date, place, region_code)) + "'"]
  echo_ps = subprocess.Popen(" ".join(command), stdout=subprocess.PIPE, universal_newlines=True, shell=True)
  output = subprocess.check_output([
      'dremel',
      "--min_completion_ratio=" + MIN_COMPLETION_RATIO,
      "--sql_dialect=GoogleSQL",
      ], stdin=echo_ps.stdout, universal_newlines=True)
  echo_ps.wait()
  lines = output.split("\n")
  data = {}
  for line in lines:
    parts = line.split("|")
    if len(parts) > 5:
      date = parts[1].strip()
      if date.startswith("2"):
        year = int(date[:4])
        month = int(date[4:6])
        day = int(date[6:8])
        month_half = str(month) + "-" + ("1" if day < 15 else "2")
        temp_min = float(parts[3].strip())
        temp_average = float(parts[4].strip())
        temp_max = float(parts[5].strip())
        rainfall = float(parts[6].strip())
        if str(year) not in data:
          data[str(year)] = {}
        if month_half not in data[str(year)]:
          data[str(year)][month_half] = []
        data[str(year)][month_half].append([temp_min, temp_average, temp_max, rainfall])
  sys.stdout.write(str(len(data)) + "y\n")
  sys.stdout.flush()
  if not len(data) and len(master_data):
    continue
  # Now we have ~15 temperature tuples (min, max, mean) for each half-month of
  # each year.
  # Let's start by reducing all this into one tuple for each half-month of each
  # year.
  for y in all_years_str:
    for month in range(1, 13):
        for suff in range(1, 3):
          month_half = str(month) + "-" + str(suff)
          if DEBUG and y in data and month_half in data[y]:
            print("Now going to average for " + month_half + " in " + y)
            print(data[y][month_half])
          # "-9999" is code for "no data"
          if y in data and month_half in data[y]:
            array_of_min_tmps = [float(p[0]) for p in data[y][month_half] if p[0] > -1000]
            array_of_average_temps = [float(p[1]) for p in data[y][month_half] if p[1] > -1000]
            array_of_max_tmps = [float(p[2]) for p in data[y][month_half] if p[2] > -1000]
            array_of_rainfall = [float(p[3]) for p in data[y][month_half] if p[3] > -1000]
            if len(array_of_max_tmps) > 0:
              max_temp_for_this_half_month = int(round(float(max(array_of_max_tmps))))
            else:
              max_temp_for_this_half_month = -9999
            if array_has_data(array_of_average_temps):
              average_temp_for_this_half_month = \
                  int(round(float(sum(array_of_average_temps)) / max(len(array_of_average_temps), 1)))
            else:
              average_temp_for_this_half_month = -9999
            if len(array_of_min_tmps) > 0:
              min_temp_for_this_half_month = int(round(float(min(array_of_min_tmps))))
            else:
              min_temp_for_this_half_month = -9999
            if array_has_data(array_of_rainfall):
              average_rainfall_for_this_half_month = \
                  int(round(float(sum(array_of_rainfall)) / max(len(array_of_rainfall), 1)))
            else:
              average_rainfall_for_this_half_month = -9999
            data[y][month_half] = [min_temp_for_this_half_month, average_temp_for_this_half_month, \
                max_temp_for_this_half_month, average_rainfall_for_this_half_month]

  for month in range(1, 13):
    for suff in range(1, 3):
      month_half = str(month) + "-" + str(suff)
      # Now we need to average everything corresponding to this half-month for
      # all our available years.
      array_of_min_temps = []
      array_of_average_temps = []
      array_of_max_temps = []
      array_of_rainfall = []
      for y in all_years_str:
        if y in data and month_half in data[y]:
          if data[y][month_half][0] > -1000:
            array_of_min_temps.append(data[y][month_half][0])
          if data[y][month_half][1] > -1000:
            array_of_average_temps.append(data[y][month_half][1])
          if data[y][month_half][2] > -1000:
            array_of_max_temps.append(data[y][month_half][2])
          if data[y][month_half][3] > -1000:
            array_of_rainfall.append(data[y][month_half][3])

      if DEBUG:
        print("Averaging the following for " + month_half + " across all years:")
        print(array_of_average_temps)
      if array_has_data(array_of_min_temps):
        min_temp_average = \
            int(round(float(sum(array_of_min_temps)) / max(len(array_of_min_temps), 1)))
      else:
        min_temp_average = -9999
      if array_has_data(array_of_average_temps):
        temp_average = \
            int(round(float(sum(array_of_average_temps)) / max(len(array_of_average_temps), 1)))
      else:
        temp_average = -9999
      if array_has_data(array_of_max_temps):
        max_temp_average = \
            int(round(float(sum(array_of_max_temps)) / max(len(array_of_max_temps), 1)))
      else:
        max_temp_average = -9999
      if array_has_data(array_of_rainfall):
        rainfall_average = \
            int(round(float(sum(array_of_rainfall)) / max(len(array_of_rainfall), 1)))
      else:
        rainfall_average = -9999
      master_data[location][month_half] = [min_temp_average, temp_average, max_temp_average, rainfall_average]
  # Done with this location
  for y in all_years_str:
    if len(master_data[location]) != 24:
      print("Warning: only got " + str(len(master_data[location])) + \
          " half-months for " + place)
serialized = ""
for location in sorted(master_data.keys()):
  serialized += location + ":" + json.dumps(master_data[location], sort_keys=True) + "\n"
open(DATA_FILE, "w").write(serialized)
print("All done.")
